{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch에서 제공하는 RNN 클래스는 기본적으로 Elman RNN 이지만,\n",
        "제공하는 클래스 대신 직접 Elman RNN을 구현한다.\n",
        "하나의 RNN 타임 스텝을 구현한 RNNCell을 사용하여 Elman RNN을 구현해보자."
      ],
      "metadata": {
        "id": "xgl8WjQfk-mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "INTAf_IUmXxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ElmanRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, batch_first=False): # input_size : 입력 벡터 크기, hidden_size : 은닉 상태 벡터 크기, batch_first : 0번째 차원이 배치인지 여부\n",
        "    super(ElmanRNN, self).__init__()\n",
        "    self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "\n",
        "    self.batch_first = batch_first  # 0번째 차원이 배치인지 여부\n",
        "    self.hidden_size = hidden_size  # 은닉 상태 벡터 크기\n",
        "\n",
        "  def _initialize_hidden(self, batch_size):\n",
        "    return torch.zeros((batch_size, self.hidden_size)) # 형상은 (N x H)가 되며, N : 배치 크기, H :은닉 상태 크기\n",
        "\n",
        "  def forward(self, x_in, initial_hidden=None):\n",
        "    '''\n",
        "    0번째 차원이 배치라면 x_in의 0번째와 1번째 순서를 바꾸어준다.\n",
        "    반대로 0번째 차원이 배치가 아닌 경우에는\n",
        "    0번째 차원 : 시퀀스 크기 (시계열 데이터의 분량)\n",
        "    1번째 차원 : 배치 크기\n",
        "    '''\n",
        "    if self.batch_first:\n",
        "      batch_size, seq_size, feat_size = x_in.size()\n",
        "      x_in = x_in.permute(1, 0, 2)\n",
        "    else:\n",
        "      seq_size, batch_size, feat_size = x_in.size()\n",
        "\n",
        "    hiddens = []\n",
        "\n",
        "    if initial_hidden is None:                    # 초기 은닉이 정해져있지 않다면, 초기화한다.\n",
        "      initial_hidden = self._initialize_hidden(batch_size)\n",
        "      initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "    hidden_t = initial_hidden\n",
        "\n",
        "    for t in range(seq_size):\n",
        "      hidden_t = self.rnn_cell(x_in[t], hidden_t) # 입력 벡터와 은닉 상태를 전달하는데, 출력된 은닉 상태는 다음의 rnn_cell에 전달된다.\n",
        "      hiddens.append(hidden_t)\n",
        "\n",
        "    hiddens = torch.stack(hiddens)\n",
        "\n",
        "    if self.batch_first:                          # 0번째 차원이 배치라면 은닉 상태 순서를 바꿔준다.\n",
        "      hiddens = hiddens.permute(1, 0, 2)\n",
        "\n",
        "    return hiddens"
      ],
      "metadata": {
        "id": "lvqI8OMllAED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자 시퀀스를 입력으로 받아 성씨에 따른 국적 분류 예측을 해보자.\n",
        "시퀀스는 다음과 같이 입력된다.\n",
        "'Davidson' : 'D','a','v','i','d','s','o','n'\n",
        "\n",
        "성씨 데이터셋 특징은 다음과 같다.\n",
        "1. 데이터 속성이 불규칙 (영어 27%, 러시아어 21%, 아랍어 14%, 나머지 38%)\n",
        "2. 출신 국가와 성씨 철자법 사이에 의미 있고 직관적 관계가 있다.\n"
      ],
      "metadata": {
        "id": "_JSFo3E9UNi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "JjLwljlXTzfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    raw_dataset_csv=\"data/surnames/surnames.csv\",\n",
        "    train_proportion=0.7\n",
        "    val_proportion=0.15,\n",
        "    test_proportion=0.15,\n",
        "    output_munged_csv=\"data/surnames/surnames_with_splits.csv\", # nationality_index, split 이 추가됨\n",
        "    # 경로 정보\n",
        "    surname_csv=\"data/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"model_storage/surname_classification\",\n",
        "    # 모델 하이퍼파라미터\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # 훈련 하이퍼파라미터\n",
        "    seed=1337,\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    early_stopping_criteria=5,\n",
        "    # 실행 옵션\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True\n",
        ")"
      ],
      "metadata": {
        "id": "49SrGUhGVFWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameDataset(Dataset):\n",
        "  def __init__(self, surname_df, vectorizer):\n",
        "    \"\"\"\n",
        "    매개변수:\n",
        "        surname_df (pandas.DataFrame): 데이터셋\n",
        "        vectorizer (SurnameVectorizer): 데이터셋에서 만든 Vectorizer 객체\n",
        "    \"\"\"\n",
        "    self.surname_df = surname_df\n",
        "    self._vectorizer = vectorizer\n",
        "\n",
        "    self._max_seq_length = max(map(len, self.surname_df.surname)) + 2 # 시퀀스의 최대 크기\n",
        "\n",
        "    self.train_df = self.surname_df[self.surname_df.split=='train']\n",
        "    self.train_size = len(self.train_df)\n",
        "\n",
        "    self.val_df = self.surname_df[self.surname_df.split=='val']\n",
        "    self.validation_size = len(self.val_df)\n",
        "\n",
        "    self.test_df = self.surname_df[self.surname_df.split=='test']\n",
        "    self.test_size = len(self.test_df)\n",
        "\n",
        "    self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                          'val': (self.val_df, self.validation_size),\n",
        "                          'test': (self.test_df, self.test_size)}\n",
        "\n",
        "    self.set_split('train')\n",
        "\n",
        "    # 클래스 가중치\n",
        "    class_counts = self.train_df.nationality.value_counts().to_dict()\n",
        "    def sort_key(item):\n",
        "        return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "    sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "    frequencies = [count for _, count in sorted_counts]\n",
        "    self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "  @classmethod\n",
        "  def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
        "    surname_df = pd.read_csv(surname_csv)\n",
        "    train_surname_df = surname_df[surname_df.split=='train']\n",
        "    return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df)) # SurnameDataset 객체 반환\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    row = self._target_df.iloc[index]\n",
        "\n",
        "    surname_vector, vec_length = self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "\n",
        "    nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "\n",
        "    return {'x_data':surname_vector,\n",
        "            'y_target':nationality_index,\n",
        "            'x_length':vec_length}\n",
        "\n",
        "  def get_num_batches(self, batch_size): # 배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수\n",
        "    return len(self) // batch_size\n"
      ],
      "metadata": {
        "id": "b1A2PLoXoFn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary 클래스를 상속 받아 SequenceVocabulary 클래스를 선언하여 활용할 것이다.\n",
        "\n",
        "SequenceVocabulary 클래스 역할\n",
        "1. 주어진 이름 시퀀스의 각 \"문자\"를 정수로 매핑\n",
        "2. 특별 토큰 활용\n"
      ],
      "metadata": {
        "id": "0lgl8Rv9tokA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "  \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
        "\n",
        "  def __init__(self, token_to_idx=None):\n",
        "    \"\"\"\n",
        "    매개변수:\n",
        "        token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
        "    \"\"\"\n",
        "\n",
        "    if token_to_idx is None:\n",
        "        token_to_idx = {}\n",
        "    self._token_to_idx = token_to_idx\n",
        "\n",
        "    self._idx_to_token = {idx: token\n",
        "                          for token, idx in self._token_to_idx.items()}\n",
        "\n",
        "  def to_serializable(self):\n",
        "    \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
        "    return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "  @classmethod\n",
        "  def from_serializable(cls, contents):\n",
        "    \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
        "    return cls(**contents)\n",
        "  def add_token(self, token):\n",
        "    \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
        "\n",
        "    매개변수:\n",
        "        token (str): Vocabulary에 추가할 토큰\n",
        "    반환값:\n",
        "        index (int): 토큰에 상응하는 정수\n",
        "    \"\"\"\n",
        "    if token in self._token_to_idx:\n",
        "        index = self._token_to_idx[token]\n",
        "    else:\n",
        "        index = len(self._token_to_idx)\n",
        "        self._token_to_idx[token] = index\n",
        "        self._idx_to_token[index] = token\n",
        "    return index\n",
        "\n",
        "  def add_many(self, tokens):\n",
        "    \"\"\"토큰 리스트를 Vocabulary에 추가합니다.\n",
        "\n",
        "    매개변수:\n",
        "        tokens (list): 문자열 토큰 리스트\n",
        "    반환값:\n",
        "        indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
        "    \"\"\"\n",
        "    return [self.add_token(token) for token in tokens]\n",
        "\n",
        "  def lookup_token(self, token):\n",
        "    \"\"\"토큰에 대응하는 인덱스를 추출합니다.\n",
        "\n",
        "    매개변수:\n",
        "        token (str): 찾을 토큰\n",
        "    반환값:\n",
        "        index (int): 토큰에 해당하는 인덱스\n",
        "    \"\"\"\n",
        "    return self._token_to_idx[token]\n",
        "\n",
        "  def lookup_index(self, index):\n",
        "    \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
        "\n",
        "    매개변수:\n",
        "        index (int): 찾을 인덱스\n",
        "    반환값:\n",
        "        token (str): 인텍스에 해당하는 토큰\n",
        "    에러:\n",
        "        KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
        "    \"\"\"\n",
        "    if index not in self._idx_to_token:\n",
        "        raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "    return self._idx_to_token[index]\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._token_to_idx)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "위에서 선언한 Vocabulary를 상속받아 SequenceVocabulary를 선언한다.\n",
        "'''\n",
        "class SequenceVocabulary(Vocabulary):\n",
        "  def __init__(self, token_to_idx=None,\n",
        "               unk_token=\"<UNK>\",\n",
        "               mask_token=\"<MASK>\",\n",
        "               begin_seq_token=\"<BEGIN>\",\n",
        "               end_seq_token=\"<END>\"):\n",
        "    super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "    self._mask_token = mask_token\n",
        "    self._unk_token = unk_token\n",
        "    self._begin_seq_token = begin_seq_token\n",
        "    self._end_seq_token = end_seq_token\n",
        "\n",
        "    self.mask_index = self.add_token(self._mask_token)\n",
        "    self.unk_index = self.add_token(self._unk_token)\n",
        "    self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
        "    self.end_seq_index = self.add_token(self._end_seq_token)\n",
        "\n",
        "  def to_serializable(self):\n",
        "    contents = super(SequenceVocabulary, self).to_serializable()\n",
        "    contents.update({'unk_token': self._unk_token,\n",
        "                     'mask_token': self._mask_token,\n",
        "                     'begin_seq_token': self._begin_seq_token,\n",
        "                     'end_seq_token': self._end_seq_token})\n",
        "    return contents\n",
        "\n",
        "  def lookup_token(self, token):\n",
        "    if self.unk_index >= 0: # UNK 토큰을 사용하려면 UNK 인덱스가 0보다 커야한다.\n",
        "      # 만약 token에 해당하는 key를 찾을 수 없으면 디폴트로 unk_index를 반환한다.\n",
        "      return self._token_to_idx.get(token, self.unk_index)\n",
        "    else: # UNK 토큰을 사용하지 않을 경우..\n",
        "      return self._token_to_idx[token]"
      ],
      "metadata": {
        "id": "zAMeNi1XtWmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SurnameVectorizer 클래스 선언\n",
        "\n",
        "SurnameVectorizer 클래스 역할 : SequenceVocabulary를 사용해 성씨에 있는 문자와 정수 간의 매핑을 관리한다"
      ],
      "metadata": {
        "id": "noDFc8Z81Zl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameVectorizer(object):\n",
        "  '''어휘 사전을 생성하고 관리한다'''\n",
        "  # surname 문자열, vector_length 인덱스 벡터 길이를 맞추기 위한 매개변수\n",
        "  def vectorize(self, surname, vector_length=-1):\n",
        "    '''입력으로 받은 성씨의 각 문자를 인덱스로 변환'''\n",
        "    # BEGIN 토큰 인덱스 추가\n",
        "    indices = [self.char_vocab.begin_seq_index]\n",
        "    # 각 문자를 인덱스로 변환\n",
        "    indices.extend(self.char_vocab, lookup_token(token) for token in surname)\n",
        "    # END 토큰 인덱스 추가\n",
        "    indices.append(self.char_vocab.end_seq_index)\n",
        "\n",
        "    # 인덱스 벡터 길이가 안주어졌다면 벡터 길이를 문자들을 변환한 인덱스 길이로 초기화\n",
        "    if vector_length < 0:\n",
        "      vector_length = len(indices)\n",
        "\n",
        "    '''\n",
        "    시퀀스 길이 +2(특수 토큰 <BEGIN>, <END>를 추가한 indices 길이)의 넘파이 벡터를 생성하고\n",
        "    각 위치의 인덱스 값을 저장\n",
        "    '''\n",
        "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "    out_vector[:len(indices)] = indices\n",
        "    # 입력 시퀀스가 vector_length보다 짧아 남는 공간은 MASK 인덱스로 패딩\n",
        "    out_vector[len(indices):] = self.char_vocab.mask_index\n",
        "\n",
        "    # 단어 인덱스를 저장한 벡터와 입력 시퀀스(시퀀스 길이+2) 길이를 반환\n",
        "    return out_vector, len(indices)\n",
        "\n",
        "  @classmethod\n",
        "  def from_dataframe(cls, surname_df): # surname_df 성씨 데이터셋의 데이터프레임\n",
        "    '''\n",
        "    데이터셋 데이터프레임으로 SurnameVectorizer 객체를 초기화한다.\n",
        "    '''\n",
        "    char_vocab = SequenceVocabulary()\n",
        "    nationality_vocab = Vocabulary()\n",
        "\n",
        "    for index, row in surname_df.iterrows(): # 성씨 데이터셋의 행 순회\n",
        "      for char in row.surname: # 성씨의 문자열 문자 순회\n",
        "        char_vocab.add_token(char) # SequenceVocabulary 객체에 토큰으로 성씨 문자 추가\n",
        "      nationality_vocab.add_token(row.nationality) # Vocabulary 객체에 토큰으로 nationality 추가\n",
        "\n",
        "    return cls(char_vocab, nationality_vocab)"
      ],
      "metadata": {
        "id": "aQYbZ0qE1Y7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SurnameClassifier 클래스 선언\n",
        "\n",
        "\n",
        "1. 임베딩 층을 통해 문자에 대한 정수들(모델 입력은 SequenceVocabulary에서 정수로 매핑한 토큰)을 임베딩한다.\n",
        "2. RNN으로 시퀀스의 벡터 표현(은닉 상태)을 계산하고, RNN을 반복적으로 거쳐 성씨를 요약한 벡터를 추출한다.\n",
        "3. 이 요약한 최종 벡터를 Linear층으로 전달하여 예측 벡터를 계산한다.\n",
        "4. 이 예측 벡터를 사용하여 손실을 계산하거나 Softmax 활성 함수에 적용하여 성씨에 대한 확률 분포를 만든다."
      ],
      "metadata": {
        "id": "QkVT1C8GEzox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def column_gather(y_out, x_lengths):\n",
        "  # TODO : y_out에 있는 각 데이터 포인트에서 마지막 벡터를 추출\n",
        "\n",
        "class SurnameClassifier(nn.Module):\n",
        "  def __init__(self, embedding_size, num_embeddings, num_classes, rnn_hidden_size, batch_first=True, padding_idx=0)\n",
        "  '''\n",
        "  매개 변수로 임베딩 크기, 임베딩 개수, 클래스 개수(예측 벡터의 크기, 국적 개수), RNN의 은닉 상태 크기가 필요\n",
        "  '''\n",
        "  super(SurnameClassifier, self).__init__()\n",
        "\n",
        "  self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                          embedding_dim=embedding_size,\n",
        "                          padding_idx=padding_idx)\n",
        "  self.rnn = EmanRNN(input_size=embedding_size,\n",
        "                     hidden_size=rnn_hidden_size,\n",
        "                     batch_first=batch_first)\n",
        "  # affine 계층\n",
        "  self.fc1 = nn.Linear(in_features=rnn_hidden_size, out_features=rnn_hidden_size)\n",
        "  self.fc2 = nn.Linear(in_features=rnn_hidden_size, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "    x_embedded = self.emb(x_in)\n",
        "    y_out = self.rnn(x_embedded)\n",
        "\n",
        "    if x_lengths is not None:\n",
        "      # x_lengths를 이용하여 rnn 계층을 통과한 마지막 출력 벡터를 구한다\n",
        "      y_out = column_gather(y_out, x_lengths) # column_gather은 각 시퀀스의 최종 출력 벡터를 추출하는 역할의 함수다.\n",
        "    else:\n",
        "      y_out = y_out[:, -1, :]\n",
        "\n",
        "    y_out = F.dropout(y_out, 0.5)\n",
        "    y_out = F.relu(self.fc1(y_out))\n",
        "    y_out = F.dropout(y_out, 0.5)\n",
        "    y_out = F.fc2(y_out)\n",
        "    if apply_softmax: # Softmax 활성화 함수를 적용할 경우 (크로스 엔트로피 손실을 사용하려면 False로 지정됨)\n",
        "      y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "    return y_out"
      ],
      "metadata": {
        "id": "9Oj1otnGEzDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : 모델 훈련 및 평가"
      ],
      "metadata": {
        "id": "7IZ3UlfgNe_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}