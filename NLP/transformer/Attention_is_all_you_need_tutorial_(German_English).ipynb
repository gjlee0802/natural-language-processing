{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is All You Need (NIPS 2017) 실습\n",
        "\n",
        "트랜스포머 정리 노트: https://github.com/gjlee0802/natural-language-processing/blob/main/NLP/transformer/attention_is_all_you_need_summary.md\n",
        "\n",
        "\n",
        "독일어를 영어로 번역하는 Machine Translation 구현, 데이터셋은 Multi30k 이용."
      ],
      "metadata": {
        "id": "f9fadiGlp5EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리(Preprocessing)\n",
        "\n",
        "spacy 라이브러리: 문장의 토큰화, 태깅 등의 전처리 기능을 위한 라이브러리\n",
        "\n",
        "영어와 독일어 전처리 모듈 설치"
      ],
      "metadata": {
        "id": "fxcMGl0fs0Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "metadata": {
        "id": "1k_hv31Ip4eu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9JExj6Ytn1RP"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_de = spacy.load('de_core_news_sm')"
      ],
      "metadata": {
        "id": "NtS7D7ZYpXvs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "  print(f\"인덱스 {i} : {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTRo5yXjphab",
        "outputId": "7b946294-0c96-4df5-9053-d7a284a5df70"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0 : I\n",
            "인덱스 1 : am\n",
            "인덱스 2 : a\n",
            "인덱스 3 : graduate\n",
            "인덱스 4 : student\n",
            "인덱스 5 : .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토큰화 함수 정의 (spacy의 토크나이저 이용)"
      ],
      "metadata": {
        "id": "WrbKUTTIsxLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 독일어 문장을 토큰화 하는 함수\n",
        "def tokenize_de(text):\n",
        "  return [token.text for token in spacy_de.tokenizer(text)]\n",
        "\n",
        "# 영어 문장을 토큰화 하는 함수\n",
        "def tokenize_en(text):\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]\n"
      ],
      "metadata": {
        "id": "0TN6TdgBscdG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZMobabcvtvo",
        "outputId": "c6e7b977-f7de-42ae-ade7-79942d8557cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.16.0\n",
        "!pip install portalocker>=2.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzZUvITRujcF",
        "outputId": "46538257-18a5-4260-86a8-d1a4e62d0112"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.16.0 in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext==0.16.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext==0.16.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 어휘집 Vocab 만들기"
      ],
      "metadata": {
        "id": "9pg6vp1wGa_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List"
      ],
      "metadata": {
        "id": "cPx1ryFGMbQ0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANG = 'de'\n",
        "TGT_LANG = 'en'"
      ],
      "metadata": {
        "id": "WXaA4kGG_Sfe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = {}\n",
        "tokenizer['en'] = tokenize_en\n",
        "tokenizer['de'] = tokenize_de"
      ],
      "metadata": {
        "id": "0xwMHiw9DEsR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 목록을 생성하기 위한 헬퍼(helper) 함수\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANG: 0, TGT_LANG: 1}\n",
        "\n",
        "    for i, datasample_tuple in enumerate(train_iter):\n",
        "      yield tokenizer[language](datasample_tuple[language_index[language]])\n",
        "    '''\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "    '''"
      ],
      "metadata": {
        "id": "uxzGsx_g_Png"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수 기호(symbol)와 인덱스를 정의합니다\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# 토큰들이 어휘집(vocab)에 인덱스 순서대로 잘 삽입되어 있는지 확인합니다\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocab_transform = {} # 영어, 독일어에 대해서 torchtext의 Vocab 옵젝이 저장됨.\n",
        "\n",
        "for ln in [SRC_LANG, TGT_LANG]:\n",
        "  train_iter = Multi30k(split='train', language_pair=(SRC_LANG, TGT_LANG))\n",
        "  val_iter = Multi30k(split='valid', language_pair=(SRC_LANG, TGT_LANG))\n",
        "  test_iter = Multi30k(split='test', language_pair=(SRC_LANG, TGT_LANG))\n",
        "\n",
        "  vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                  min_freq=2, # 최소 2번 이상 등장한 단어만을 선택\n",
        "                                                  specials=special_symbols,\n",
        "                                                  special_first=True)"
      ],
      "metadata": {
        "id": "ivS0lAYgMWZf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ``UNK_IDX`` 를 기본 인덱스로 설정합니다. 이 인덱스는 토큰을 찾지 못하는 경우에 반환됩니다.\n",
        "# 만약 기본 인덱스를 설정하지 않으면 어휘집(Vocabulary)에서 토큰을 찾지 못하는 경우\n",
        "# ``RuntimeError`` 가 발생합니다.\n",
        "for ln in [SRC_LANG, TGT_LANG]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "aTp2GdU4BIN5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('소스 언어의 Vocab(어휘집)')\n",
        "for idx in range(20):\n",
        "  print(f'[Vocab] index: {idx} | token: {vocab_transform[SRC_LANG].lookup_token(idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNAwtyBLEqc6",
        "outputId": "bb13eae9-7a50-4348-ae5d-f5e34caecea9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 언어의 Vocab(어휘집)\n",
            "[Vocab] index: 0 | token: <unk>\n",
            "[Vocab] index: 1 | token: <pad>\n",
            "[Vocab] index: 2 | token: <bos>\n",
            "[Vocab] index: 3 | token: <eos>\n",
            "[Vocab] index: 4 | token: .\n",
            "[Vocab] index: 5 | token: Ein\n",
            "[Vocab] index: 6 | token: einem\n",
            "[Vocab] index: 7 | token: in\n",
            "[Vocab] index: 8 | token: ,\n",
            "[Vocab] index: 9 | token: und\n",
            "[Vocab] index: 10 | token: mit\n",
            "[Vocab] index: 11 | token: auf\n",
            "[Vocab] index: 12 | token: Mann\n",
            "[Vocab] index: 13 | token: einer\n",
            "[Vocab] index: 14 | token: Eine\n",
            "[Vocab] index: 15 | token: ein\n",
            "[Vocab] index: 16 | token: der\n",
            "[Vocab] index: 17 | token: Frau\n",
            "[Vocab] index: 18 | token: eine\n",
            "[Vocab] index: 19 | token: die\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('타겟 언어의 Vocab(어휘집)')\n",
        "for idx in range(20):\n",
        "  print(f'[Vocab] index: {idx} | token: {vocab_transform[TGT_LANG].lookup_token(idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h16VuyrcF7ae",
        "outputId": "6be1338f-78d7-41cf-89d1-50ef33372077"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "타겟 언어의 Vocab(어휘집)\n",
            "[Vocab] index: 0 | token: <unk>\n",
            "[Vocab] index: 1 | token: <pad>\n",
            "[Vocab] index: 2 | token: <bos>\n",
            "[Vocab] index: 3 | token: <eos>\n",
            "[Vocab] index: 4 | token: a\n",
            "[Vocab] index: 5 | token: .\n",
            "[Vocab] index: 6 | token: A\n",
            "[Vocab] index: 7 | token: in\n",
            "[Vocab] index: 8 | token: the\n",
            "[Vocab] index: 9 | token: on\n",
            "[Vocab] index: 10 | token: is\n",
            "[Vocab] index: 11 | token: and\n",
            "[Vocab] index: 12 | token: man\n",
            "[Vocab] index: 13 | token: of\n",
            "[Vocab] index: 14 | token: with\n",
            "[Vocab] index: 15 | token: ,\n",
            "[Vocab] index: 16 | token: woman\n",
            "[Vocab] index: 17 | token: are\n",
            "[Vocab] index: 18 | token: to\n",
            "[Vocab] index: 19 | token: Two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab_transform[SRC_LANG]))\n",
        "print(len(vocab_transform[TGT_LANG]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_zPx6xLnsh",
        "outputId": "bb684c6a-71e1-4144-c0ab-9e4f0d3a73e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8014\n",
            "6191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer 활용 Seq2Seq 모델\n",
        "\n",
        "torch에서 제공하는 Transformer을 사용하지 않고, transformer을 구현하여 활용해보자."
      ],
      "metadata": {
        "id": "r86krVyZGiGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Apd2N7M2GrSD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "q6Bu7xTaJPrO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "어텐션은 세가지 요소를 입력으로 받는다.\n",
        "- 쿼리(queries)\n",
        "- 키(keys)\n",
        "- 값(values)\n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_heads: 헤드의 개수(scaled dot-product attention 개수)\n",
        "- dropout_ratio: 드롭아웃 비율"
      ],
      "metadata": {
        "id": "wRVJ_C0XPNbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    # assert는 뒤의 조건이 True가 아니면 AssertError를 발생한다.\n",
        "    assert hidden_dim % n_heads == 0\n",
        "\n",
        "    self.hidden_dim = hidden_dim # 임베딩 차원\n",
        "    self.n_heads = n_heads # 헤드의 개수(서로 다른 어텐션 컨셉의 수)\n",
        "    self.head_dim = hidden_dim // n_heads # 각 헤드에서의 임베딩 차원 = 전체 임베딩 차원을 헤드의 수로 나눈 값\n",
        "\n",
        "    self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
        "    self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
        "    self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
        "\n",
        "    self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    # query: [batch_size, query_len, hidden_dim]\n",
        "    # key: [batch_size, key_len, hidden_dim]\n",
        "    # value: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "    # 각각 FC 레이어에 입력\n",
        "    Q = self.fc_q(query)\n",
        "    K = self.fc_k(key)\n",
        "    V = self.fc_v(value)\n",
        "\n",
        "    # Q: [batch_size, query_len, hidden_dim]\n",
        "    # K: [batch_size, key_len, hidden_dim]\n",
        "    # V: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "    # hidden_dim -> n_heads X head_dim 형태로 변형\n",
        "    # after permute\n",
        "    # Q: [batch_size, query_len, n_heads, head_dim] -> [batch_size, n_heads, query_len, head_dim]\n",
        "    # K: [batch_size, key_len, n_heads, head_dim] -> [batch_size, n_heads, key_len, head_dim]\n",
        "    # V: [batch_size, value_len, n_heads, head_dim] -> [batch_size, n_heads, value_len, head_dim]\n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "    # Q: [batch_size, n_heads, query_len, head_dim]\n",
        "    # K: [batch_size, n_heads, key_len, head_dim]\n",
        "    # V: [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "    # Attention Energy 계산 (유사도 계산)\n",
        "    energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "    # 마스크를 사용할 경우\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask==0, -1e10) # 마스크 값이 0인 부분에 상당이 작은 값으로 채워준다.\n",
        "\n",
        "    # 어텐션 스코어 계산: 각 단어에 대한 확률 값\n",
        "    attention = torch.softmax(energy, dim=-1) # 소프트맥스로 정규화\n",
        "\n",
        "    # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "    # Scaled Dot-Product Attention을 계산\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "    # x: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "    x = x.permute(0,2,1,3).contiguous()\n",
        "\n",
        "    # x: [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "    # n_heads X head_dim -> hidden_dim 변형\n",
        "    x = x.view(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "    # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "    x = self.fc_o(x)\n",
        "\n",
        "    return x, attention\n"
      ],
      "metadata": {
        "id": "7A6B9TajPSvm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position-wise Feedforward\n",
        "입력과 출력의 차원이 동일함.  \n",
        "- encoder와 decoder의 각각의 layer는 fully connected feed-forward network를 포함하고 있음.  \n",
        "- position 마다, 즉 개별 단어마다 적용되기 때문에 position-wise.  \n",
        "- network는 두 번의 linear transformation과 activation function ReLU로 이루어져 있음(fc1 -> relu -> fc2).  \n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율"
      ],
      "metadata": {
        "id": "WrHqPNDD1ib3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "      super().__init__()\n",
        "\n",
        "      self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
        "      self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "    x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "    # x: [batch_size, seq_len, pf_dim]\n",
        "\n",
        "    x = self.fc_2(x)\n",
        "\n",
        "    # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "RM2bCgO-1IDQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder 레이어\n",
        "인코더는 아래의 인코더 레이어를 여러번 중첩하여 사용함.  \n",
        "인코더 레이어의 입력과 출력의 차원이 같음.  \n",
        "\n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_heads: 헤드의 개수\n",
        "- pf_dim: Feedforward 레이어(PositionwiseFeedforward)에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율"
      ],
      "metadata": {
        "id": "UpCBRc9A4DO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "    # src_mask: [batch_size, src_len]\n",
        "\n",
        "    # Self Attention\n",
        "    # 필요한 경우 마스크 행렬을 이용하여 어텐션할 단어 조절 가능\n",
        "    _src, _ = self.self_attention.forward(src, src, src, src_mask) # params : query, key, value, mask\n",
        "\n",
        "    # dropout, residual connection and layer norm\n",
        "    # residual connection : feedforward를 거치기 전 입력 x를 feedforward를 거친 결과값에 더해주어 입력하는 것\n",
        "    src = self.self_atten_layer_norm.forward(src + self.dropout(_src))\n",
        "\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    # Position-wisd feedforward\n",
        "    _src = self.posittionwise_feedforward.forward(src)\n",
        "\n",
        "    # dropout, residual and layer norm\n",
        "    src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    return src"
      ],
      "metadata": {
        "id": "0_NDqu9B4IEJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n",
        "\n",
        "하이퍼 파라미터\n",
        "- input_dim: 하나의 단어에 대한 원-핫 인코딩 차원\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_layers: 내부적으로 사용할 인코더 레이어의 개수\n",
        "- n_heads: 헤드의 개수\n",
        "- pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율\n",
        "- max_length: 문장 내 최대 단어 개수"
      ],
      "metadata": {
        "id": "vxkzEBsYxNaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "    self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    # src: [batch_size, src_len]\n",
        "    # src_mask: [batch_size, src_len]\n",
        "\n",
        "    batch_size = src.shape[0]\n",
        "    src_len = src.shape[1]\n",
        "\n",
        "    # unsqueeze는 특정 위치에 1인 차원을 추가함.\n",
        "    # unsqueeze(0)는 첫번째 차원에 1인 차원을 추가함. [1 X src_len]\n",
        "    # repeat 함수는 텐서를 반복 확장시켜줌.\n",
        "    # repeat(batch_size, 1)은 [1 X src_len] 형태를 [batch_size, src_len] 차원으로 만들어줌.\n",
        "    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "    # pos: [batch_size, src_len]\n",
        "\n",
        "    # 소스 문장의 임베딩과 위치 임베딩을 더함. (Positional Encoding)\n",
        "    src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    # 모든 인코더 레이어를 차례대로 거치며 순전파 수행\n",
        "    for layer in self.encoder_layers:\n",
        "      src = layer(src, src_mask)\n",
        "\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    # 마지막 레이어의 출력 반환\n",
        "    return src"
      ],
      "metadata": {
        "id": "75t0GSZ793lU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder 레이어\n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_heads: 헤드의 개수\n",
        "- pf_dim: Feedforward 레이어(PositionwiseFeedforward)에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율"
      ],
      "metadata": {
        "id": "Qic-2XOFMPJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  # 인코더의 출력 값(enc_src)를 어텐션하는 구조\n",
        "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "    # enc_src: [batch_size, src_len, hidden_dim]\n",
        "    # trg_mask: [batch_size, trg_len, hidden_dim]\n",
        "    # src_mask: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    _trg, _ = self.self_attention.forward(trg, trg, trg, trg_mask) # params : query, key, value, mask\n",
        "\n",
        "    trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "    # positionwise feedforward\n",
        "    _trg = self.positionwise_feedforward(trg)\n",
        "\n",
        "    # dropout, residual and layer norm\n",
        "    # residual connection : feedforward를 거치기 전 입력 x를 feedforward를 거친 결과값에 더해주어 입력하는 것\n",
        "    trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "\n",
        "    return trg, attention"
      ],
      "metadata": {
        "id": "YX1nh8YWMOFl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "원본 논문과 다르게 위치 임베딩(positional embedding)을 학습하는 형태(BERT와 같은 모던 트랜스포머 모델에서 사용되는 방식)로 구현함.  \n",
        "  \n",
        "소스 문장의 'pad' 토큰에 대해 마스크(MASK) 값을 0으로 설정함.  \n",
        "  \n",
        "Masked Decoder Self-Attention: 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크(MASK)를 사용함.\n",
        "~~~\n",
        "Masked Decoder Self-Attention : 디코더 파트에서 셀프 어텐션을 사용할 때는 각각의 출력 단어가 다른 모든 출력 단어를 참고하도록 하지는 않고, 앞쪽의 단어들만 참고하도록 함.\n",
        "~~~\n",
        "\n",
        "하이퍼 파라미터\n",
        "- output_dim: 하나의 단어에 대한 원-핫 인코딩 차원\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_layers: 내부적으로 사용할 인코더 레이어의 개수\n",
        "- n_heads: 헤드의 개수\n",
        "- pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율\n",
        "- max_length: 문장 내 최대 단어 개수"
      ],
      "metadata": {
        "id": "seOLTrmBR7eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "    self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "    self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "    self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "    # trg: [batch_size, trg_len]\n",
        "    # enc_src: [batch_size, src_len, hidden_dim]\n",
        "    # trg_mask: [batch_size, trg_len]\n",
        "    # src_mask: [batch_size, src_len]\n",
        "\n",
        "    batch_size = trg.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    # unsqueeze는 특정 위치에 1인 차원을 추가함.\n",
        "    # unsqueeze(0)는 첫번째 차원에 1인 차원을 추가함. [1 X trg_len]\n",
        "    # repeat 함수는 텐서를 반복 확장시켜줌.\n",
        "    # repeat(batch_size, 1)은 [1 X trg_len] 형태를 [batch_size, trg_len] 차원으로 만들어줌.\n",
        "    pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "    # pos : [batch_size, trg_len]\n",
        "\n",
        "    # 타겟 문장의 임베딩과 위치 임베딩을 더함. (Positional Encoding)\n",
        "    trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "    # 모든 디코더 레이어를 거치며 순전파 수행\n",
        "    for layer in self.decoder_layers:\n",
        "      trg, attention = layer(trg, enc_src, trg_mask, src_mask) # 소스 마스크와 타겟 마스크 모두 사용\n",
        "\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "    # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "    output = self.fc_out(trg)\n",
        "\n",
        "    # output: [batch_size, trg_len, output_dim]\n",
        "\n",
        "    return output, attention"
      ],
      "metadata": {
        "id": "fQ48w4KqSoei"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model\n",
        "입력이 들어왔을 때 앞서 정의한 Encoder와 Decoder을 거쳐 출력 문장을 생성함.\n",
        "\n",
        "파라미터\n",
        "- encoder: encoder 객체\n",
        "- decoder: decoder 객체\n",
        "- src_pad_idx: 소스 문장의 패딩 문자 인덱스\n",
        "- trg_pad_idx: 타겟 문장의 패딩 문자 인덱스"
      ],
      "metadata": {
        "id": "A1BSe_vRZZIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "\n",
        "    # src: [batch_size, src_len]\n",
        "\n",
        "    # unsqueeze를 사용하여 1번째 자리, 2번째 자리에 1인 차원을 추가한다.\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # src_mask: [batch_size, 1, 1, src_len]\n",
        "\n",
        "    return src_mask\n",
        "\n",
        "  def make_trg_mask(self, trg):\n",
        "\n",
        "    # trg: [batch_size, trg_len]\n",
        "\n",
        "    trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
        "\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "\n",
        "    # trg_sub_mask: [trg_len, trg_len] # 정방 행렬 텐서\n",
        "\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "    # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "    return trg_mask\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    # src: [batch_size, src_len]\n",
        "    # trg: [batch_size, trg_len]\n",
        "\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "    # src_mask: [batch_size, 1, 1, src_len]\n",
        "    # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "    # enc_src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "    # output: [batch_size, trg_len, otuput_dim]\n",
        "    # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "    return output, attention\n"
      ],
      "metadata": {
        "id": "m5rahb6AZhok"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training (학습)"
      ],
      "metadata": {
        "id": "RbKLabhmgErU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 하이퍼 파라미터 설정"
      ],
      "metadata": {
        "id": "K5p-kCLmmspl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab_transform[SRC_LANG])\n",
        "OUTPUT_DIM = len(vocab_transform[TGT_LANG])\n",
        "HIDDEN_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "A9dXwLSDZj4p"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 초기화"
      ],
      "metadata": {
        "id": "4xqGYQMamnjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 인덱스를 확인하기 위해 출력해본다.\n",
        "print(vocab_transform[SRC_LANG].lookup_indices(['<pad>']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAJbQwfsiM8I",
        "outputId": "79d5eaf5-5890-4977-be81-254d9dc18afd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_PAD_IDX = vocab_transform[SRC_LANG].lookup_indices(['<pad>'])\n",
        "TGT_PAD_IDX = vocab_transform[TGT_LANG].lookup_indices(['<pad>'])\n",
        "\n",
        "# 인코더와 디코더 객체 선언\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
        "\n",
        "# Transformer 객체 선언\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TGT_PAD_IDX, device).to(device)"
      ],
      "metadata": {
        "id": "gpuMMWv1h7Q2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 가중치 파라미터 초기화"
      ],
      "metadata": {
        "id": "SH6p7N-FnGS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLXaoHqsjp1k",
        "outputId": "362ed631-8a52-46d5-9366-8971d89aa629"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 9,232,431 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data) # 가중치를 Xavier 값으로 초기화\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqcMJz4UnyXk",
        "outputId": "796d0c58-1923-4c51-ede0-be2421078725"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(8014, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0-2): 3 x EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(6191, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (decoder_layers): ModuleList(\n",
              "      (0-2): 3 x DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=6191, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 및 평가 함수 정의"
      ],
      "metadata": {
        "id": "Ih-oGnXtn8qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer는 Adam optimizer 사용"
      ],
      "metadata": {
        "id": "PGuSujTzpTvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 뒷 부분의 패딩에 대해서는 값 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)"
      ],
      "metadata": {
        "id": "P_1PEe_PpLWy"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습 함수"
      ],
      "metadata": {
        "id": "jDKQoU_lpoLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "jtJZjeGjsKR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}