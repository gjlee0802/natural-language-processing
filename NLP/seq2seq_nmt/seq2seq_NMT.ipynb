{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dIlwJLJAjRdD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary와 SequenceVocabulary"
      ],
      "metadata": {
        "id": "KAfe9ysEnx8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "  \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
        "\n",
        "  def __init__(self, token_to_idx=None):\n",
        "      \"\"\"\n",
        "      매개변수:\n",
        "          token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
        "      \"\"\"\n",
        "\n",
        "      if token_to_idx is None:\n",
        "          token_to_idx = {}\n",
        "      self._token_to_idx = token_to_idx\n",
        "\n",
        "      self._idx_to_token = {idx: token\n",
        "                            for token, idx in self._token_to_idx.items()}\n",
        "\n",
        "  def to_serializable(self):\n",
        "      \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
        "      return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "  @classmethod\n",
        "  def from_serializable(cls, contents):\n",
        "      \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
        "      return cls(**contents)\n",
        "\n",
        "  def add_token(self, token):\n",
        "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
        "\n",
        "        매개변수:\n",
        "            token (str): Vocabulary에 추가할 토큰\n",
        "        반환값:\n",
        "            index (int): 토큰에 상응하는 정수\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "  def add_many(self, tokens):\n",
        "      \"\"\"토큰 리스트를 Vocabulary에 추가합니다.\n",
        "\n",
        "      매개변수:\n",
        "          tokens (list): 문자열 토큰 리스트\n",
        "      반환값:\n",
        "          indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
        "      \"\"\"\n",
        "      return [self.add_token(token) for token in tokens]\n",
        "\n",
        "  def lookup_token(self, token):\n",
        "      \"\"\"토큰에 대응하는 인덱스를 추출합니다.\n",
        "\n",
        "      매개변수:\n",
        "          token (str): 찾을 토큰\n",
        "      반환값:\n",
        "          index (int): 토큰에 해당하는 인덱스\n",
        "      \"\"\"\n",
        "      return self._token_to_idx[token]\n",
        "\n",
        "  def lookup_index(self, index):\n",
        "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
        "\n",
        "        매개변수:\n",
        "            index (int): 찾을 인덱스\n",
        "        반환값:\n",
        "            token (str): 인텍스에 해당하는 토큰\n",
        "        에러:\n",
        "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "  def __str__(self):\n",
        "      return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self._token_to_idx)\n"
      ],
      "metadata": {
        "id": "5phhFez5mhiD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "  def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                end_seq_token=\"<END>\"):\n",
        "\n",
        "      super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "      self._mask_token = mask_token\n",
        "      self._unk_token = unk_token\n",
        "      self._begin_seq_token = begin_seq_token\n",
        "      self._end_seq_token = end_seq_token\n",
        "\n",
        "      self.mask_index = self.add_token(self._mask_token)\n",
        "      self.unk_index = self.add_token(self._unk_token)\n",
        "      self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
        "      self.end_seq_index = self.add_token(self._end_seq_token)\n",
        "\n",
        "  def to_serializable(self):\n",
        "      contents = super(SequenceVocabulary, self).to_serializable()\n",
        "      contents.update({'unk_token': self._unk_token,\n",
        "                        'mask_token': self._mask_token,\n",
        "                        'begin_seq_token': self._begin_seq_token,\n",
        "                        'end_seq_token': self._end_seq_token})\n",
        "      return contents\n",
        "\n",
        "  def lookup_token(self, token):\n",
        "      \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
        "      토큰이 없으면 UNK 인덱스를 반환합니다.\n",
        "\n",
        "      매개변수:\n",
        "          token (str): 찾을 토큰\n",
        "      반환값:\n",
        "          index (int): 토큰에 해당하는 인덱스\n",
        "      노트:\n",
        "          UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
        "          `unk_index`가 0보다 커야 합니다.\n",
        "      \"\"\"\n",
        "      if self.unk_index >= 0:\n",
        "          return self._token_to_idx.get(token, self.unk_index)\n",
        "      else:\n",
        "          return self._token_to_idx[token]"
      ],
      "metadata": {
        "id": "hOzDdfLwnJq8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorizer"
      ],
      "metadata": {
        "id": "Z9MbGMIin5Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTVectorizer(object):\n",
        "  def __init__(self, source_vocab, max_source_length, max_target_length):\n",
        "    '''\n",
        "    source_vocab : 소스 단어 정수 매핑\n",
        "    target_vocab : 타깃 단어 정수 매핑\n",
        "    max_source_length : 소스 데이터셋에서 가장 긴 시퀀스 길이\n",
        "    max_target_length : 타깃 데이터셋에서 가장 긴 시퀀스 길이\n",
        "    '''\n",
        "\n",
        "    self.source_vocab = source_vocab\n",
        "    self.max_source_length = max_source_length\n",
        "    self.max_target_length = max_target_length\n",
        "\n",
        "  def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
        "    '''\n",
        "    index를 벡터로 변환\n",
        "\n",
        "    indices : 시퀀스를 나타내는 정수 리스트\n",
        "    vector_length : 인덱스 벡터 길이\n",
        "    mask_index : 사용할 MASK 인덱스\n",
        "    '''\n",
        "\n",
        "    if vector_length < 0:             # vector_length 파라미터 전달이 안됐을 경우, indices길이로 지정\n",
        "      vector_length = len(indices)\n",
        "\n",
        "    vector = np.zeros(vector_length, dtype=np.int64)\n",
        "    vector[:len(indices)] = indices\n",
        "    vector[len(indices):] = mask_index  # MASK 인덱스로 패딩\n",
        "\n",
        "    return vector\n",
        "\n",
        "  def _get_source_indices(self, text):\n",
        "    '''\n",
        "    벡터로 변환된 소스 텍스트를 반환 (소스 텍스트의 벡터를 반환)\n",
        "    '''\n",
        "    indices = [self.source_vocab.begin_seq_index]\n",
        "    indices.extend(self.source_vocab.lookup_token(token) for token in text.split(\" \")) # token의 인덱스를 찾음\n",
        "    indices.append(self.source_vocab.end_seq_index)\n",
        "    return indices\n",
        "\n",
        "  def _get_target_indices(self, text):\n",
        "    '''\n",
        "    벡터로 변환된 타깃 텍스트를 반환\n",
        "    '''\n",
        "\n",
        "    indices = [self.target_vocab.lookup_token(token) for token in text.split(' ')]\n",
        "    x_indices = [self.target_vocab.begin_seq_index] + indices\n",
        "    y_indices = indices + [self.taget_vocab.end_seq_index]\n",
        "    return x_indices, y_indices # x_indices : 디코더에서 샘플을 나타내는 정수 리스트, y_indices : 디코더에서 예측을 나타내는 정수 리스트\n",
        "\n",
        "  def vectorize(self, source_text, target_text, use_dataset_max_lengths=True):\n",
        "    '''\n",
        "    벡터화된 소스 텍스트와 타깃 텍스트 반환\n",
        "    '''\n",
        "    source_vector_length = -1\n",
        "    target_vector_length = -1\n",
        "\n",
        "    if use_dataset_max_lengths:\n",
        "      source_vector_length = self.max_source_length + 2\n",
        "      target_vector_length = self.max_target_length + 1\n",
        "\n",
        "    source_indices = self._get_source_indices(source_text)\n",
        "    source_vector = self._vectorize(source_indices,\n",
        "                                    vector_length=source_vector_length,\n",
        "                                    mask_index=self.source_vocab.mask_index)\n",
        "\n",
        "    target_x_indices, target_y_indices = self._get_target_indices(target_text)\n",
        "    target_x_vector = self._vectorize(target_x_indices,\n",
        "                                      vector_length=target_vector_length,\n",
        "                                      mask_index=self.target_vocab.mask_index)\n",
        "    target_y_vector = self._vectorize(target_y_indices,\n",
        "                                      vector_length=target_vector_length,\n",
        "                                      mask_index=self.target_vocab.mask_index)\n",
        "    return {\n",
        "        \"source_vector\":source_vector,\n",
        "        \"target_x_vector\":target_x_vector,\n",
        "        \"target_y_vector\":target_y_vector,\n",
        "        \"source_length\":len(source_indices)\n",
        "    }\n",
        "\n",
        "  @classmethod\n",
        "  def from_dataframe(cls, bitext_df):\n",
        "    '''\n",
        "    데이터셋 데이터프레임으로 Vectorizer 초기화\n",
        "    '''\n",
        "\n",
        "    source_vocab = SequenceVocabulary()\n",
        "    target_vocab = SequenceVocabulary()\n",
        "\n",
        "    max_source_length, max_target_length = 0, 0\n",
        "\n",
        "    for _, row in bitext_df.iterrows():\n",
        "      source_tokens = row[\"source_language\"].split(' ')\n",
        "      if len(source_tokens) > max_source_length:\n",
        "        max_source_length = len(source_tokens)\n",
        "      for token in source_tokens:\n",
        "        source_vocab.add_token(token)\n",
        "\n",
        "      target_tokens = row[\"target_language\"].split(' ')\n",
        "      if len(target_tokens) > max_target_length:\n",
        "        max_target_length = len(target_tokens)\n",
        "      for token in target_tokens:\n",
        "        target_vocab.add_token(token)\n",
        "\n",
        "    return cls(source_vocab, max_source_length, max_target_length)"
      ],
      "metadata": {
        "id": "-BYRphlMn9G0"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}