{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리(Preprocessing)\n",
        "\n",
        "spacy 라이브러리: 문장의 토큰화, 태깅 등의 전처리 기능을 위한 라이브러리\n",
        "\n",
        "영어와 독일어 전처리 모듈 설치"
      ],
      "metadata": {
        "id": "fxcMGl0fs0Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "metadata": {
        "id": "1k_hv31Ip4eu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9JExj6Ytn1RP"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_de = spacy.load('de_core_news_sm')"
      ],
      "metadata": {
        "id": "NtS7D7ZYpXvs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "  print(f\"인덱스 {i} : {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTRo5yXjphab",
        "outputId": "e3b89401-edaf-43ea-8581-bf82b547584e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0 : I\n",
            "인덱스 1 : am\n",
            "인덱스 2 : a\n",
            "인덱스 3 : graduate\n",
            "인덱스 4 : student\n",
            "인덱스 5 : .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토큰화 함수 정의 (spacy의 토크나이저 이용)"
      ],
      "metadata": {
        "id": "WrbKUTTIsxLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 독일어 문장을 토큰화 하는 함수\n",
        "def tokenize_de(text):\n",
        "  return [token.text for token in spacy_de.tokenizer(text)]\n",
        "\n",
        "# 영어 문장을 토큰화 하는 함수\n",
        "def tokenize_en(text):\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]\n"
      ],
      "metadata": {
        "id": "0TN6TdgBscdG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZMobabcvtvo",
        "outputId": "d0ebc9d0-4094-4ec1-c16d-42456244b749"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.16.0\n",
        "!pip install portalocker>=2.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzZUvITRujcF",
        "outputId": "8ba4e3c5-8654-4350-e776-872deedc44d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.16.0 in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext==0.16.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext==0.16.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 어휘집 Vocab 만들기"
      ],
      "metadata": {
        "id": "9pg6vp1wGa_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List"
      ],
      "metadata": {
        "id": "cPx1ryFGMbQ0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANG = 'de'\n",
        "TGT_LANG = 'en'"
      ],
      "metadata": {
        "id": "WXaA4kGG_Sfe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = {}\n",
        "tokenizer['en'] = tokenize_en\n",
        "tokenizer['de'] = tokenize_de"
      ],
      "metadata": {
        "id": "0xwMHiw9DEsR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 목록을 생성하기 위한 헬퍼(helper) 함수\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANG: 0, TGT_LANG: 1}\n",
        "\n",
        "    for i, datasample_tuple in enumerate(train_iter):\n",
        "      yield tokenizer[language](datasample_tuple[language_index[language]])\n",
        "    '''\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "    '''"
      ],
      "metadata": {
        "id": "uxzGsx_g_Png"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수 기호(symbol)와 인덱스를 정의합니다\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# 토큰들이 어휘집(vocab)에 인덱스 순서대로 잘 삽입되어 있는지 확인합니다\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocab_transform = {} # 영어, 독일어에 대해서 torchtext의 Vocab 옵젝이 저장됨.\n",
        "\n",
        "for ln in [SRC_LANG, TGT_LANG]:\n",
        "  train_iter = Multi30k(split='train', language_pair=(SRC_LANG, TGT_LANG))\n",
        "  val_iter = Multi30k(split='valid', language_pair=(SRC_LANG, TGT_LANG))\n",
        "  test_iter = Multi30k(split='test', language_pair=(SRC_LANG, TGT_LANG))\n",
        "\n",
        "  vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                  min_freq=2, # 최소 2번 이상 등장한 단어만을 선택\n",
        "                                                  specials=special_symbols,\n",
        "                                                  special_first=True)"
      ],
      "metadata": {
        "id": "ivS0lAYgMWZf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ``UNK_IDX`` 를 기본 인덱스로 설정합니다. 이 인덱스는 토큰을 찾지 못하는 경우에 반환됩니다.\n",
        "# 만약 기본 인덱스를 설정하지 않으면 어휘집(Vocabulary)에서 토큰을 찾지 못하는 경우\n",
        "# ``RuntimeError`` 가 발생합니다.\n",
        "for ln in [SRC_LANG, TGT_LANG]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "aTp2GdU4BIN5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('소스 언어의 Vocab(어휘집)')\n",
        "for idx in range(20):\n",
        "  print(f'[Vocab] index: {idx} | token: {vocab_transform[SRC_LANG].lookup_token(idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNAwtyBLEqc6",
        "outputId": "7e5224f5-4515-489c-90ee-f31e22b6dedf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 언어의 Vocab(어휘집)\n",
            "[Vocab] index: 0 | token: <unk>\n",
            "[Vocab] index: 1 | token: <pad>\n",
            "[Vocab] index: 2 | token: <bos>\n",
            "[Vocab] index: 3 | token: <eos>\n",
            "[Vocab] index: 4 | token: .\n",
            "[Vocab] index: 5 | token: Ein\n",
            "[Vocab] index: 6 | token: einem\n",
            "[Vocab] index: 7 | token: in\n",
            "[Vocab] index: 8 | token: ,\n",
            "[Vocab] index: 9 | token: und\n",
            "[Vocab] index: 10 | token: mit\n",
            "[Vocab] index: 11 | token: auf\n",
            "[Vocab] index: 12 | token: Mann\n",
            "[Vocab] index: 13 | token: einer\n",
            "[Vocab] index: 14 | token: Eine\n",
            "[Vocab] index: 15 | token: ein\n",
            "[Vocab] index: 16 | token: der\n",
            "[Vocab] index: 17 | token: Frau\n",
            "[Vocab] index: 18 | token: eine\n",
            "[Vocab] index: 19 | token: die\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('타겟 언어의 Vocab(어휘집)')\n",
        "for idx in range(20):\n",
        "  print(f'[Vocab] index: {idx} | token: {vocab_transform[TGT_LANG].lookup_token(idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h16VuyrcF7ae",
        "outputId": "26579423-7810-45c2-d7d8-f641f0b8132e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "타겟 언어의 Vocab(어휘집)\n",
            "[Vocab] index: 0 | token: <unk>\n",
            "[Vocab] index: 1 | token: <pad>\n",
            "[Vocab] index: 2 | token: <bos>\n",
            "[Vocab] index: 3 | token: <eos>\n",
            "[Vocab] index: 4 | token: a\n",
            "[Vocab] index: 5 | token: .\n",
            "[Vocab] index: 6 | token: A\n",
            "[Vocab] index: 7 | token: in\n",
            "[Vocab] index: 8 | token: the\n",
            "[Vocab] index: 9 | token: on\n",
            "[Vocab] index: 10 | token: is\n",
            "[Vocab] index: 11 | token: and\n",
            "[Vocab] index: 12 | token: man\n",
            "[Vocab] index: 13 | token: of\n",
            "[Vocab] index: 14 | token: with\n",
            "[Vocab] index: 15 | token: ,\n",
            "[Vocab] index: 16 | token: woman\n",
            "[Vocab] index: 17 | token: are\n",
            "[Vocab] index: 18 | token: to\n",
            "[Vocab] index: 19 | token: Two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab_transform[SRC_LANG]))\n",
        "print(len(vocab_transform[TGT_LANG]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_zPx6xLnsh",
        "outputId": "968cb334-3b17-470a-9416-7abe2ab75592"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8014\n",
            "6191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer 활용 Seq2Seq 모델\n",
        "\n",
        "torch에서 제공하는 Transformer을 사용하지 않고, transformer을 구현하여 활용해보자."
      ],
      "metadata": {
        "id": "r86krVyZGiGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Apd2N7M2GrSD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "q6Bu7xTaJPrO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "어텐션은 세가지 요소를 입력으로 받는다.\n",
        "- 쿼리(queries)\n",
        "- 키(keys)\n",
        "- 값(values)\n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_heads: 헤드의 개수(scaled dot-product attention 개수)\n",
        "- dropout_ratio: 드롭아웃 비율"
      ],
      "metadata": {
        "id": "wRVJ_C0XPNbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    # assert는 뒤의 조건이 True가 아니면 AssertError를 발생한다.\n",
        "    assert hidden_dim % n_heads == 0\n",
        "\n",
        "    self.hidden_dim = hidden_dim # 임베딩 차원\n",
        "    self.n_heads = n_heads # 헤드의 개수(서로 다른 어텐션 컨셉의 수)\n",
        "    self.head_dim = hidden_dim // n_heads # 각 헤드에서의 임베딩 차원 = 전체 임베딩 차원을 헤드의 수로 나눈 값\n",
        "\n",
        "    self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
        "    self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
        "    self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
        "\n",
        "    self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    # query: [batch_size, query_len, hidden_dim]\n",
        "    # key: [batch_size, key_len, hidden_dim]\n",
        "    # value: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "    # 각각 FC 레이어에 입력\n",
        "    Q = self.fc_q(query)\n",
        "    K = self.fc_k(key)\n",
        "    V = self.fc_v(value)\n",
        "\n",
        "    # Q: [batch_size, query_len, hidden_dim]\n",
        "    # K: [batch_size, key_len, hidden_dim]\n",
        "    # V: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "    # hidden_dim -> n_heads X head_dim 형태로 변형\n",
        "    # after permute\n",
        "    # Q: [batch_size, query_len, n_heads, head_dim] -> [batch_size, n_heads, query_len, head_dim]\n",
        "    # K: [batch_size, key_len, n_heads, head_dim] -> [batch_size, n_heads, key_len, head_dim]\n",
        "    # V: [batch_size, value_len, n_heads, head_dim] -> [batch_size, n_heads, value_len, head_dim]\n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "    # Q: [batch_size, n_heads, query_len, head_dim]\n",
        "    # K: [batch_size, n_heads, key_len, head_dim]\n",
        "    # V: [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "    # Attention Energy 계산 (유사도 계산)\n",
        "    energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "    # 마스크를 사용할 경우\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask==0, -1e10) # 마스크 값이 0인 부분에 상당이 작은 값으로 채워준다.\n",
        "\n",
        "    # 어텐션 스코어 계산: 각 단어에 대한 확률 값\n",
        "    attention = torch.softmax(energy, dim=-1) # 소프트맥스로 정규화\n",
        "\n",
        "    # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "    # Scaled Dot-Product Attention을 계산\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "    # x: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "    x = x.permute(0,2,1,3).contiguous()\n",
        "\n",
        "    # x: [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "    # n_heads X head_dim -> hidden_dim 변형\n",
        "    x = x.view(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "    # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "    x = self.fc_o(x)\n",
        "\n",
        "    return x, attention\n"
      ],
      "metadata": {
        "id": "7A6B9TajPSvm"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}