{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fadiGlp5EO"
      },
      "source": [
        "# Attention is All You Need (NIPS 2017) 실습\n",
        "\n",
        "트랜스포머 정리 노트: https://github.com/gjlee0802/natural-language-processing/blob/main/NLP/transformer/attention_is_all_you_need_summary.md\n",
        "\n",
        "\n",
        "독일어를 영어로 번역하는 Machine Translation 구현, 데이터셋은 Multi30k 이용."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxcMGl0fs0Eq"
      },
      "source": [
        "# 데이터 전처리(Preprocessing)\n",
        "\n",
        "spacy 라이브러리: 문장의 토큰화, 태깅 등의 전처리 기능을 위한 라이브러리\n",
        "\n",
        "영어와 독일어 전처리 모듈 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1k_hv31Ip4eu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9JExj6Ytn1RP"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NtS7D7ZYpXvs"
      },
      "outputs": [],
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_de = spacy.load('de_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTRo5yXjphab",
        "outputId": "92cf501f-5172-47cd-ebc2-3df4fc42e4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0 : I\n",
            "인덱스 1 : am\n",
            "인덱스 2 : a\n",
            "인덱스 3 : graduate\n",
            "인덱스 4 : student\n",
            "인덱스 5 : .\n"
          ]
        }
      ],
      "source": [
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "  print(f\"인덱스 {i} : {token.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrbKUTTIsxLV"
      },
      "source": [
        "## 토큰화 함수 정의 (spacy의 토크나이저 이용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0TN6TdgBscdG"
      },
      "outputs": [],
      "source": [
        "# 독일어 문장을 토큰화 하는 함수\n",
        "def tokenize_de(text):\n",
        "  return [token.text for token in spacy_de.tokenizer(text)]\n",
        "\n",
        "# 영어 문장을 토큰화 하는 함수\n",
        "def tokenize_en(text):\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QMbeaaRXBaK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZMobabcvtvo",
        "outputId": "69fa3122-9b7e-4027-a826-7bc790e1f238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzZUvITRujcF",
        "outputId": "7cd95bef-a5ed-4c4f-aaee-40d511698190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.16.0 in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext==0.16.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext==0.16.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.16.0\n",
        "!pip install portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pg6vp1wGa_v"
      },
      "source": [
        "## 어휘집 Vocab 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cPx1ryFGMbQ0"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WXaA4kGG_Sfe"
      },
      "outputs": [],
      "source": [
        "SRC_LANG = 'de'\n",
        "TGT_LANG = 'en'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0xwMHiw9DEsR"
      },
      "outputs": [],
      "source": [
        "tokenizer = {}\n",
        "tokenizer['en'] = tokenize_en\n",
        "tokenizer['de'] = tokenize_de"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uxzGsx_g_Png"
      },
      "outputs": [],
      "source": [
        "# 토큰 목록을 생성하기 위한 헬퍼(helper) 함수\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANG: 0, TGT_LANG: 1}\n",
        "\n",
        "    for i, datasample_tuple in enumerate(train_iter):\n",
        "      yield tokenizer[language](datasample_tuple[language_index[language]])\n",
        "    '''\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ivS0lAYgMWZf"
      },
      "outputs": [],
      "source": [
        "# 특수 기호(symbol)와 인덱스를 정의합니다\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# 토큰들이 어휘집(vocab)에 인덱스 순서대로 잘 삽입되어 있는지 확인합니다\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocab_transform = {} # 영어, 독일어에 대해서 torchtext의 Vocab 옵젝이 저장됨.\n",
        "\n",
        "for ln in [SRC_LANG, TGT_LANG]:\n",
        "  train_iter = Multi30k(split='train', language_pair=(SRC_LANG, TGT_LANG))\n",
        "  val_iter = Multi30k(split='valid', language_pair=(SRC_LANG, TGT_LANG))\n",
        "  test_iter = Multi30k(split='test', language_pair=(SRC_LANG, TGT_LANG))\n",
        "\n",
        "  vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                  min_freq=2, # 최소 2번 이상 등장한 단어만을 선택\n",
        "                                                  specials=special_symbols,\n",
        "                                                  special_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9K165Ma_MPg",
        "outputId": "ce614653-42c9-41c6-c3a5-1d5a98cb4d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] \n",
            "x:Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche. \n",
            "y:Two young, White males are outside near many bushes.\n",
            "[1] \n",
            "x:Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem. \n",
            "y:Several men in hard hats are operating a giant pulley system.\n",
            "[2] \n",
            "x:Ein kleines Mädchen klettert in ein Spielhaus aus Holz. \n",
            "y:A little girl climbing into a wooden playhouse.\n",
            "[3] \n",
            "x:Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster. \n",
            "y:A man in a blue shirt is standing on a ladder cleaning a window.\n",
            "[4] \n",
            "x:Zwei Männer stehen am Herd und bereiten Essen zu. \n",
            "y:Two men are at the stove preparing food.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "for idx, (x, y) in enumerate(train_iter):\n",
        "  if idx == 5:\n",
        "    break\n",
        "\n",
        "  print(f'[{idx}] \\nx:{x} \\ny:{y}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aTp2GdU4BIN5"
      },
      "outputs": [],
      "source": [
        "# ``UNK_IDX`` 를 기본 인덱스로 설정합니다. 이 인덱스는 토큰을 찾지 못하는 경우에 반환됩니다.\n",
        "# 만약 기본 인덱스를 설정하지 않으면 어휘집(Vocabulary)에서 토큰을 찾지 못하는 경우\n",
        "# ``RuntimeError`` 가 발생합니다.\n",
        "for ln in [SRC_LANG, TGT_LANG]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNAwtyBLEqc6",
        "outputId": "8440582e-5ca6-411a-a69a-4ece3486dd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 언어의 Vocab(어휘집)\n",
            "[Vocab] index: 0 | token: <unk>\n",
            "[Vocab] index: 1 | token: <pad>\n",
            "[Vocab] index: 2 | token: <bos>\n",
            "[Vocab] index: 3 | token: <eos>\n",
            "[Vocab] index: 4 | token: .\n",
            "[Vocab] index: 5 | token: Ein\n",
            "[Vocab] index: 6 | token: einem\n",
            "[Vocab] index: 7 | token: in\n",
            "[Vocab] index: 8 | token: ,\n",
            "[Vocab] index: 9 | token: und\n",
            "[Vocab] index: 10 | token: mit\n",
            "[Vocab] index: 11 | token: auf\n",
            "[Vocab] index: 12 | token: Mann\n",
            "[Vocab] index: 13 | token: einer\n",
            "[Vocab] index: 14 | token: Eine\n",
            "[Vocab] index: 15 | token: ein\n",
            "[Vocab] index: 16 | token: der\n",
            "[Vocab] index: 17 | token: Frau\n",
            "[Vocab] index: 18 | token: eine\n",
            "[Vocab] index: 19 | token: die\n"
          ]
        }
      ],
      "source": [
        "print('소스 언어의 Vocab(어휘집)')\n",
        "for idx in range(20):\n",
        "  print(f'[Vocab] index: {idx} | token: {vocab_transform[SRC_LANG].lookup_token(idx)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h16VuyrcF7ae",
        "outputId": "7dd49686-3bae-4993-f237-a68695bf24f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "타겟 언어의 Vocab(어휘집)\n",
            "[Vocab] index: 0 | token: <unk>\n",
            "[Vocab] index: 1 | token: <pad>\n",
            "[Vocab] index: 2 | token: <bos>\n",
            "[Vocab] index: 3 | token: <eos>\n",
            "[Vocab] index: 4 | token: a\n",
            "[Vocab] index: 5 | token: .\n",
            "[Vocab] index: 6 | token: A\n",
            "[Vocab] index: 7 | token: in\n",
            "[Vocab] index: 8 | token: the\n",
            "[Vocab] index: 9 | token: on\n",
            "[Vocab] index: 10 | token: is\n",
            "[Vocab] index: 11 | token: and\n",
            "[Vocab] index: 12 | token: man\n",
            "[Vocab] index: 13 | token: of\n",
            "[Vocab] index: 14 | token: with\n",
            "[Vocab] index: 15 | token: ,\n",
            "[Vocab] index: 16 | token: woman\n",
            "[Vocab] index: 17 | token: are\n",
            "[Vocab] index: 18 | token: to\n",
            "[Vocab] index: 19 | token: Two\n"
          ]
        }
      ],
      "source": [
        "print('타겟 언어의 Vocab(어휘집)')\n",
        "for idx in range(20):\n",
        "  print(f'[Vocab] index: {idx} | token: {vocab_transform[TGT_LANG].lookup_token(idx)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9Mzk1E-FAUs",
        "outputId": "f380e3a9-bf12-4dba-b861-9883a4f4bd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6]\n"
          ]
        }
      ],
      "source": [
        "print(vocab_transform[TGT_LANG].lookup_indices(['A']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_zPx6xLnsh",
        "outputId": "6fb05ef8-d319-4c08-b0d6-535f97249d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8014\n",
            "6191\n"
          ]
        }
      ],
      "source": [
        "print(len(vocab_transform[SRC_LANG]))\n",
        "print(len(vocab_transform[TGT_LANG]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q6Bu7xTaJPrO"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치 생성 전처리 코드 추가"
      ],
      "metadata": {
        "id": "9PPOjrn-SBMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "v24L26f-C4zi"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import pad\n",
        "\n",
        "src_pipeline = lambda x: vocab_transform[SRC_LANG].lookup_indices(tokenizer[SRC_LANG](x))\n",
        "tgt_pipeline = lambda x: vocab_transform[TGT_LANG].lookup_indices(tokenizer[TGT_LANG](x))\n",
        "\n",
        "def collate_batch(batch):\n",
        "\n",
        "  bs_id = torch.tensor([BOS_IDX])\n",
        "  eos_id = torch.tensor([EOS_IDX])\n",
        "\n",
        "  src_list, tgt_list = [], []\n",
        "  for (_srctext, _tgttext) in batch:\n",
        "    processed_src = torch.cat(\n",
        "        [\n",
        "            bs_id,\n",
        "            torch.tensor(\n",
        "                src_pipeline(_srctext),\n",
        "                dtype=torch.int64\n",
        "            ),\n",
        "            eos_id,\n",
        "        ],\n",
        "        0,\n",
        "    )\n",
        "    processed_tgt = torch.cat(\n",
        "        [\n",
        "            bs_id,\n",
        "            torch.tensor(\n",
        "                tgt_pipeline(_tgttext),\n",
        "                dtype=torch.int64\n",
        "            ),\n",
        "            eos_id\n",
        "        ],\n",
        "        0,\n",
        "    )\n",
        "    src_list.append(\n",
        "            # warning - overwrites values for negative values of padding - len\n",
        "            pad(\n",
        "                processed_src,\n",
        "                (\n",
        "                    0,\n",
        "                    BATCH_SIZE - len(processed_src),\n",
        "                ),\n",
        "                value=PAD_IDX,\n",
        "            )\n",
        "    )\n",
        "    tgt_list.append(\n",
        "            # warning - overwrites values for negative values of padding - len\n",
        "            pad(\n",
        "                processed_src,\n",
        "                (\n",
        "                    0,\n",
        "                    BATCH_SIZE - len(processed_src),\n",
        "                ),\n",
        "                value=PAD_IDX,\n",
        "            )\n",
        "    )\n",
        "\n",
        "  src = torch.stack(src_list)\n",
        "  tgt = torch.stack(tgt_list)\n",
        "  return src, tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j9QkUCNh5ouZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def train_valid_split(train_iterator, split_ratio=0.8, seed=42):\n",
        "    train_count = int(split_ratio * len(train_iterator))\n",
        "    valid_count = len(train_iterator) - train_count\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_set, valid_set = random_split(\n",
        "        train_iterator, lengths=[train_count, valid_count], generator=generator)\n",
        "    return train_set, valid_set\n",
        "\n",
        "# iterable type에서 map style로 변환해야 length check 가능\n",
        "train_iter = to_map_style_dataset(train_iter)\n",
        "valid_iter = to_map_style_dataset(val_iter)\n",
        "#train_set, valid_set = train_valid_split(train_iter)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn = collate_batch)\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn = collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJwkeOUoiY-y",
        "outputId": "c72c6565-0624-4520-87b3-3eaec52cb424"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(valid_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_WY977piby3",
        "outputId": "f6ba1ccd-b267-4095-906c-4fa7bc9432d7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPiTz04K_asj"
      },
      "source": [
        "train_dataloader를 돌며 Source에 대한 배치 데이터 출력해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r86krVyZGiGa"
      },
      "source": [
        "# Transformer 활용 Seq2Seq 모델\n",
        "\n",
        "torch에서 제공하는 Transformer을 사용하지 않고, transformer을 구현하여 활용해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Apd2N7M2GrSD"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRVJ_C0XPNbG"
      },
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "어텐션은 세가지 요소를 입력으로 받는다.\n",
        "- 쿼리(queries)\n",
        "- 키(keys)\n",
        "- 값(values)\n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_heads: 헤드의 개수(scaled dot-product attention 개수)\n",
        "- dropout_ratio: 드롭아웃 비율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7A6B9TajPSvm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    # assert는 뒤의 조건이 True가 아니면 AssertError를 발생한다.\n",
        "    assert hidden_dim % n_heads == 0\n",
        "\n",
        "    self.hidden_dim = hidden_dim # 임베딩 차원\n",
        "    self.n_heads = n_heads # 헤드의 개수(서로 다른 어텐션 컨셉의 수)\n",
        "    self.head_dim = hidden_dim // n_heads # 각 헤드에서의 임베딩 차원 = 전체 임베딩 차원을 헤드의 수로 나눈 값\n",
        "\n",
        "    self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
        "    self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
        "    self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
        "\n",
        "    self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    # query: [batch_size, query_len, hidden_dim]\n",
        "    # key: [batch_size, key_len, hidden_dim]\n",
        "    # value: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "    # 각각 FC 레이어에 입력\n",
        "    Q = self.fc_q(query)\n",
        "    K = self.fc_k(key)\n",
        "    V = self.fc_v(value)\n",
        "\n",
        "    # Q: [batch_size, query_len, hidden_dim]\n",
        "    # K: [batch_size, key_len, hidden_dim]\n",
        "    # V: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "    # hidden_dim -> n_heads X head_dim 형태로 변형\n",
        "    # after permute\n",
        "    # Q: [batch_size, query_len, n_heads, head_dim] -> [batch_size, n_heads, query_len, head_dim]\n",
        "    # K: [batch_size, key_len, n_heads, head_dim] -> [batch_size, n_heads, key_len, head_dim]\n",
        "    # V: [batch_size, value_len, n_heads, head_dim] -> [batch_size, n_heads, value_len, head_dim]\n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "    # Q: [batch_size, n_heads, query_len, head_dim]\n",
        "    # K: [batch_size, n_heads, key_len, head_dim]\n",
        "    # V: [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "    # Attention Energy 계산 (유사도 계산)\n",
        "    energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "    # 마스크를 사용할 경우\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask==0, -1e10) # 마스크 값이 0인 부분에 상당이 작은 값으로 채워준다.\n",
        "\n",
        "    # 어텐션 스코어 계산: 각 단어에 대한 확률 값\n",
        "    attention = torch.softmax(energy, dim=-1) # 소프트맥스로 정규화\n",
        "\n",
        "    # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "    # Scaled Dot-Product Attention을 계산\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "    # x: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "    x = x.permute(0,2,1,3).contiguous()\n",
        "\n",
        "    # x: [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "    # n_heads X head_dim -> hidden_dim 변형\n",
        "    x = x.view(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "    # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "    x = self.fc_o(x)\n",
        "\n",
        "    return x, attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrHqPNDD1ib3"
      },
      "source": [
        "## Position-wise Feedforward\n",
        "입력과 출력의 차원이 동일함.  \n",
        "- encoder와 decoder의 각각의 layer는 fully connected feed-forward network를 포함하고 있음.  \n",
        "- position 마다, 즉 개별 단어마다 적용되기 때문에 position-wise.  \n",
        "- network는 두 번의 linear transformation과 activation function ReLU로 이루어져 있음(fc1 -> relu -> fc2).  \n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RM2bCgO-1IDQ"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "      super().__init__()\n",
        "\n",
        "      self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
        "      self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "    x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "    # x: [batch_size, seq_len, pf_dim]\n",
        "\n",
        "    x = self.fc_2(x)\n",
        "\n",
        "    # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpCBRc9A4DO5"
      },
      "source": [
        "## Encoder 레이어\n",
        "인코더는 아래의 인코더 레이어를 여러번 중첩하여 사용함.  \n",
        "인코더 레이어의 입력과 출력의 차원이 같음.  \n",
        "\n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_heads: 헤드의 개수\n",
        "- pf_dim: Feedforward 레이어(PositionwiseFeedforward)에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0_NDqu9B4IEJ"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "    # src_mask: [batch_size, src_len]\n",
        "\n",
        "    # Self Attention\n",
        "    # 필요한 경우 마스크 행렬을 이용하여 어텐션할 단어 조절 가능\n",
        "    _src, _ = self.self_attention.forward(src, src, src, src_mask) # params : query, key, value, mask\n",
        "\n",
        "    # dropout, residual connection and layer norm\n",
        "    # residual connection : feedforward를 거치기 전 입력 x를 feedforward를 거친 결과값에 더해주어 입력하는 것\n",
        "    src = self.self_atten_layer_norm.forward(src + self.dropout(_src))\n",
        "\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    # Position-wisd feedforward\n",
        "    _src = self.posittionwise_feedforward.forward(src)\n",
        "\n",
        "    # dropout, residual and layer norm\n",
        "    src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxkzEBsYxNaW"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "하이퍼 파라미터\n",
        "- input_dim: 하나의 단어에 대한 원-핫 인코딩 차원\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_layers: 내부적으로 사용할 인코더 레이어의 개수\n",
        "- n_heads: 헤드의 개수\n",
        "- pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율\n",
        "- max_length: 문장 내 최대 단어 개수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "75t0GSZ793lU"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "    self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    # src: [batch_size, src_len]\n",
        "    # src_mask: [batch_size, src_len]\n",
        "\n",
        "    batch_size = src.shape[0]\n",
        "    src_len = src.shape[1]\n",
        "\n",
        "    # unsqueeze는 특정 위치에 1인 차원을 추가함.\n",
        "    # unsqueeze(0)는 첫번째 차원에 1인 차원을 추가함. [1 X src_len]\n",
        "    # repeat 함수는 텐서를 반복 확장시켜줌.\n",
        "    # repeat(batch_size, 1)은 [1 X src_len] 형태를 [batch_size, src_len] 차원으로 만들어줌.\n",
        "    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "    # pos: [batch_size, src_len]\n",
        "\n",
        "    # 소스 문장의 임베딩과 위치 임베딩을 더함. (Positional Encoding)\n",
        "    src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    # 모든 인코더 레이어를 차례대로 거치며 순전파 수행\n",
        "    for layer in self.encoder_layers:\n",
        "      src = layer(src, src_mask)\n",
        "\n",
        "    # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    # 마지막 레이어의 출력 반환\n",
        "    return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qic-2XOFMPJV"
      },
      "source": [
        "## Decoder 레이어\n",
        "\n",
        "하이퍼 파라미터\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_heads: 헤드의 개수\n",
        "- pf_dim: Feedforward 레이어(PositionwiseFeedforward)에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YX1nh8YWMOFl"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  # 인코더의 출력 값(enc_src)를 어텐션하는 구조\n",
        "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "    # enc_src: [batch_size, src_len, hidden_dim]\n",
        "    # trg_mask: [batch_size, trg_len, hidden_dim]\n",
        "    # src_mask: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    _trg, _ = self.self_attention.forward(trg, trg, trg, trg_mask) # params : query, key, value, mask\n",
        "\n",
        "    trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "    # positionwise feedforward\n",
        "    _trg = self.positionwise_feedforward(trg)\n",
        "\n",
        "    # dropout, residual and layer norm\n",
        "    # residual connection : feedforward를 거치기 전 입력 x를 feedforward를 거친 결과값에 더해주어 입력하는 것\n",
        "    trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "\n",
        "    return trg, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seOLTrmBR7eA"
      },
      "source": [
        "## Decoder\n",
        "원본 논문과 다르게 위치 임베딩(positional embedding)을 학습하는 형태(BERT와 같은 모던 트랜스포머 모델에서 사용되는 방식)로 구현함.  \n",
        "  \n",
        "소스 문장의 'pad' 토큰에 대해 마스크(MASK) 값을 0으로 설정함.  \n",
        "  \n",
        "Masked Decoder Self-Attention: 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크(MASK)를 사용함.\n",
        "~~~\n",
        "Masked Decoder Self-Attention : 디코더 파트에서 셀프 어텐션을 사용할 때는 각각의 출력 단어가 다른 모든 출력 단어를 참고하도록 하지는 않고, 앞쪽의 단어들만 참고하도록 함.\n",
        "~~~\n",
        "\n",
        "하이퍼 파라미터\n",
        "- output_dim: 하나의 단어에 대한 원-핫 인코딩 차원\n",
        "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "- n_layers: 내부적으로 사용할 인코더 레이어의 개수\n",
        "- n_heads: 헤드의 개수\n",
        "- pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "- dropout_ratio: 드롭아웃 비율\n",
        "- max_length: 문장 내 최대 단어 개수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fQ48w4KqSoei"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "    self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "    self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "    self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "    # trg: [batch_size, trg_len]\n",
        "    # enc_src: [batch_size, src_len, hidden_dim]\n",
        "    # trg_mask: [batch_size, trg_len]\n",
        "    # src_mask: [batch_size, src_len]\n",
        "\n",
        "    batch_size = trg.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    # unsqueeze는 특정 위치에 1인 차원을 추가함.\n",
        "    # unsqueeze(0)는 첫번째 차원에 1인 차원을 추가함. [1 X trg_len]\n",
        "    # repeat 함수는 텐서를 반복 확장시켜줌.\n",
        "    # repeat(batch_size, 1)은 [1 X trg_len] 형태를 [batch_size, trg_len] 차원으로 만들어줌.\n",
        "    pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "    # pos : [batch_size, trg_len]\n",
        "\n",
        "    # 타겟 문장의 임베딩과 위치 임베딩을 더함. (Positional Encoding)\n",
        "    trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "    # 모든 디코더 레이어를 거치며 순전파 수행\n",
        "    for layer in self.decoder_layers:\n",
        "      trg, attention = layer(trg, enc_src, trg_mask, src_mask) # 소스 마스크와 타겟 마스크 모두 사용\n",
        "\n",
        "    # trg: [batch_size, trg_len, hidden_dim]\n",
        "    # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "    output = self.fc_out(trg)\n",
        "\n",
        "    # output: [batch_size, trg_len, output_dim]\n",
        "\n",
        "    return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1BSe_vRZZIY"
      },
      "source": [
        "## Transformer Model\n",
        "입력이 들어왔을 때 앞서 정의한 Encoder와 Decoder을 거쳐 출력 문장을 생성함.\n",
        "\n",
        "파라미터\n",
        "- encoder: encoder 객체\n",
        "- decoder: decoder 객체\n",
        "- src_pad_idx: 소스 문장의 패딩 문자 인덱스\n",
        "- trg_pad_idx: 타겟 문장의 패딩 문자 인덱스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "m5rahb6AZhok"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    #self.src_pad_idx = src_pad_idx\n",
        "    #self.trg_pad_idx = trg_pad_idx\n",
        "    self.padding_idx = src_pad_idx\n",
        "\n",
        "    self.device = device\n",
        "  '''\n",
        "  def make_src_mask(self, src):\n",
        "\n",
        "    # src: [batch_size, src_len]\n",
        "\n",
        "    # unsqueeze를 사용하여 1번째 자리, 2번째 자리에 1인 차원을 추가한다.\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # src_mask: [batch_size, 1, 1, src_len]\n",
        "\n",
        "    return src_mask\n",
        "\n",
        "  def make_trg_mask(self, trg):\n",
        "\n",
        "    # trg: [batch_size, trg_len]\n",
        "\n",
        "    trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
        "\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "\n",
        "    # trg_sub_mask: [trg_len, trg_len] # 정방 행렬 텐서\n",
        "\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "    # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "    return trg_mask\n",
        "  '''\n",
        "  def make_padding_mask(self, q, k):\n",
        "    # q,k의 size = (batch_size, seq_len)\n",
        "    _, q_seq_len = q.size()\n",
        "    _, k_seq_len = k.size()\n",
        "\n",
        "    q = q.ne(self.padding_idx)  # padding token을 0, 나머지를 1로 만들어줌\n",
        "    q = q.unsqueeze(1).unsqueeze(3) # (batch_size, 1, q_seq_len, 1)\n",
        "    q = q.repeat(1,1,1,k_seq_len)   # (batch_size, 1, q_seq_len, k_seq_len)\n",
        "\n",
        "    k = k.ne(self.padding_idx)\n",
        "    k = k.unsqueeze(1).unsqueeze(2) # (batch_size, 1, 1, k_seq_len)\n",
        "    k = k.repeat(1,1,q_seq_len,1)   # (batch_size, 1, q_seq_len, k_seq_len)\n",
        "\n",
        "    # and 연산\n",
        "    # (batch_size, 1, q_seq_len, k_seq_len)\n",
        "    mask = q & k\n",
        "\n",
        "    return mask\n",
        "  def make_look_ahead_mask(self, tgt):\n",
        "    _, seq_len = tgt.size()\n",
        "\n",
        "    # torch.tril 함수를 사용하여 한칸씩 밀려나며 마스킹을 해줌\n",
        "    # (seq_len, seq_len)\n",
        "    mask = torch.tril(torch.ones(seq_len,seq_len)).type(torch.BoolTensor).to(self.device)\n",
        "\n",
        "    return mask\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    # src: [batch_size, src_len]\n",
        "    # trg: [batch_size, trg_len]\n",
        "\n",
        "    '''\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "    # src_mask: [batch_size, 1, 1, src_len]\n",
        "    # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "    '''\n",
        "\n",
        "    src_mask = self.make_padding_mask(src, src)                                 # padding_mask\n",
        "    enc_dec_padding_mask = self.make_padding_mask(trg, src)                     # enc_dec_padding_mask\n",
        "    trg_mask = self.make_padding_mask(trg, trg) * self.make_look_ahead_mask(trg)# look_ahead_mask\n",
        "\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "    # enc_src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    output, attention = self.decoder(trg, enc_src, trg_mask, enc_dec_padding_mask)\n",
        "\n",
        "    # output: [batch_size, trg_len, otuput_dim]\n",
        "    # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "    return output, attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbKLabhmgErU"
      },
      "source": [
        "## Training (학습)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5p-kCLmmspl"
      },
      "source": [
        "### 하이퍼 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "A9dXwLSDZj4p"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(vocab_transform[SRC_LANG])\n",
        "OUTPUT_DIM = len(vocab_transform[TGT_LANG])\n",
        "HIDDEN_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xqGYQMamnjK"
      },
      "source": [
        "### 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAJbQwfsiM8I",
        "outputId": "15b3308f-67f7-477b-e6c8-4ce1d13f9eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ],
      "source": [
        "# 패딩 인덱스를 확인하기 위해 출력해본다.\n",
        "print(vocab_transform[SRC_LANG].lookup_indices(['<pad>']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gpuMMWv1h7Q2"
      },
      "outputs": [],
      "source": [
        "SRC_PAD_IDX = vocab_transform[SRC_LANG].lookup_indices(['<pad>'])\n",
        "TGT_PAD_IDX = vocab_transform[TGT_LANG].lookup_indices(['<pad>'])\n",
        "\n",
        "# 인코더와 디코더 객체 선언\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
        "\n",
        "# Transformer 객체 선언\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX[0], TGT_PAD_IDX[0], device).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH6p7N-FnGS3"
      },
      "source": [
        "### 모델 가중치 파라미터 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLXaoHqsjp1k",
        "outputId": "27e49d10-60d8-4bba-e649-61eb12cd5dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 9,232,431 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqcMJz4UnyXk",
        "outputId": "acb93ebd-98b0-4460-9177-12f1b47a98a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(8014, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0-2): 3 x EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(6191, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (decoder_layers): ModuleList(\n",
              "      (0-2): 3 x DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=6191, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data) # 가중치를 Xavier 값으로 초기화\n",
        "\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih-oGnXtn8qg"
      },
      "source": [
        "### 학습 및 평가 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGuSujTzpTvh"
      },
      "source": [
        "optimizer는 Adam optimizer 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "P_1PEe_PpLWy"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 뒷 부분의 패딩에 대해서는 값 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDKQoU_lpoLe"
      },
      "source": [
        "모델 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEK93Pq_YX9Q",
        "outputId": "57c046af-a5e2-443a-979e-fc91f2758384"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gW-VkN2jKTRz"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for index, batch in enumerate(dataloader):\n",
        "    src = batch[0]\n",
        "    tgt = batch[1]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 출력 단어의 마지막 인덱스()는 제외\n",
        "    # 입력을 할 때는 부터 시작하도록 처리\n",
        "    output, _ = model(src, tgt[:,:-1])\n",
        "\n",
        "    # output: [배치 크기, tgt_len - 1, output_dim]\n",
        "    # trg: [배치 크기, tgt_len]\n",
        "\n",
        "    output_dim = output.shape[-1]\n",
        "\n",
        "    output = output.contiguous().view(-1, output_dim)\n",
        "    # 출력 단어의 인덱스 0()은 제외\n",
        "    tgt = tgt[:,1:].contiguous().view(-1)\n",
        "\n",
        "    # output: [배치 크기 * tgt_len - 1, output_dim]\n",
        "    # tgt: [배치 크기 * tgt_len - 1]\n",
        "\n",
        "    # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "    loss = criterion(output, tgt)\n",
        "    loss.backward() # 기울기(gradient) 계산\n",
        "\n",
        "    # 기울기(gradient) clipping 진행\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "    # 파라미터 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    # 전체 손실 값 계산\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            src = batch[0]\n",
        "            tgt = batch[1]\n",
        "\n",
        "            # 출력 단어의 마지막 인덱스()는 제외\n",
        "            # 입력을 할 때는 부터 시작하도록 처리\n",
        "            output, _ = model(src, tgt[:,:-1])\n",
        "\n",
        "            # output: [배치 크기, trg_len - 1, output_dim]\n",
        "            # tgt: [배치 크기, tgt_len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            # 출력 단어의 인덱스 0()은 제외\n",
        "            tgt = tgt[:,1:].contiguous().view(-1)\n",
        "\n",
        "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "            # tgt: [배치 크기 * tgt_len - 1]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, tgt)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "_9lXdn2eYgTK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        ""
      ],
      "metadata": {
        "id": "ckMu93wrYp-W"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "uWP6bMABYryG",
        "outputId": "ad25f604-3dc9-443c-ab82-3dca8670338f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "128\n",
            "128\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-a667cdd4a7f1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 시작 시간 기록\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-920af38fc9aa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 출력 단어의 마지막 인덱스()는 제외\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 입력을 할 때는 부터 시작하도록 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# output: [배치 크기, tgt_len - 1, output_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-1ad35f74eadf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_look_ahead_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# look_ahead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0menc_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# enc_src: [batch_size, src_len, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-e27363b104f2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# 소스 문장의 임베딩과 위치 임베딩을 더함. (Positional Encoding)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# src: [batch_size, src_len, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}